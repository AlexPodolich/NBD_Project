{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8Sr1e8qfP0z",
        "outputId": "60e94999-8109-4dae-88eb-0185e66a705a"
      },
      "outputs": [],
      "source": [
        "%pip install -U spacy\n",
        "%pip install -U wordcloud\n",
        "%pip install -U keras\n",
        "%pip install -U seaborn\n",
        "%pip install -U tensorflow\n",
        "%pip install -U matplotlib\n",
        "%pip install -U pandas\n",
        "%pip install -U scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ra9SRuTGpu6V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as P\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "import keras\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import pad_sequences, to_categorical\n",
        "tqdm.pandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkN7OyIWtJKo"
      },
      "outputs": [],
      "source": [
        "TRAINDATA_FILE_PATH = '../dataSample/train_dataset.csv'\n",
        "print(\"Reading data from file: \", TRAINDATA_FILE_PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv50p9gJsD2M"
      },
      "outputs": [],
      "source": [
        "TRAINDATA_FILE_PATH = '../dataSample/train_dataset.csv'\n",
        "print(\"Reading data from file: \", TRAINDATA_FILE_PATH)\n",
        "\n",
        "names=['text', 'label']\n",
        "try:\n",
        "    df = pd.read_csv(TRAINDATA_FILE_PATH, names=names)\n",
        "    df.sample(5)\n",
        "    df.info()\n",
        "    df.groupby('text').nunique()\n",
        "    df['label'] = df['label'].map({0: 'Negative', 1: 'Positive'})\n",
        "    sns.countplot(x='label',data=df)\n",
        "    print(\"Data read successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {TRAINDATA_FILE_PATH} was not found.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "    print(\"Error: The file is empty.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RNZqirHG2hlT"
      },
      "outputs": [],
      "source": [
        "## Importing old jrd file\n",
        "\n",
        "def clean_data(data, re_letters):\n",
        "    data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "    data['text'] = data['text'].apply(lambda x: re_letters.sub('', x))\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join(filter(lambda x: x not in nlp.Defaults.stop_words, text.split()))\n",
        "\n",
        "\n",
        "def lemmatize(text):\n",
        "    return ' '.join([x.lemma_ for x in nlp(text)])\n",
        "\n",
        "\n",
        "def build_wordcloud(data, label_type, max_words, c_width, c_height):\n",
        "    text = ' '.join(data['text'][data['label'] == label_type])\n",
        "    wc = WordCloud(max_words=max_words, width=c_width, height=c_height, collocations=False).generate(text)\n",
        "    return wc.to_image()\n",
        "\n",
        "y = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "oDvefC8usIeU",
        "outputId": "2aaf69cb-ad4c-40d0-cbae-4d6a7829ca21"
      },
      "outputs": [],
      "source": [
        "data=df[['text','label']]\n",
        "re_letters=re.compile(r\"[^a-zA-Z\\s']\")\n",
        "\n",
        "clean_data(data, re_letters)\n",
        "data.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dzj978gmswFV"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_md',disable=['ner', 'parser'])\n",
        "nlp.add_pipe('sentencizer')\n",
        "nlp.Defaults.stop_words.add(\"game\")\n",
        "nlp.Defaults.stop_words.add(\"play\")\n",
        "nlp.Defaults.stop_words.add(\"t\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "YKCLrS-uyQrj",
        "outputId": "908d42fa-490d-4153-ac3e-d73f3ca4d9d2"
      },
      "outputs": [],
      "source": [
        "data['text']=data['text'].apply(remove_stopwords)\n",
        "data.sample(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['text']=data['text'].progress_apply(lemmatize)\n",
        "data.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "JY6ahmqLsxU9",
        "outputId": "7e23e645-5f11-4162-fc8b-2fe3bd236239"
      },
      "outputs": [],
      "source": [
        "build_wordcloud(data, 'Negative', 1000, 1600, 800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vzn0dF0_sysF"
      },
      "outputs": [],
      "source": [
        "build_wordcloud(data, 'Positive', 1000, 1600, 800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhQM5HnP4crM",
        "outputId": "9915d051-462e-48fb-82e3-2f3d5d806204"
      },
      "outputs": [],
      "source": [
        "data\n",
        "max_words = 5000\n",
        "max_len = 500\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(data.text)\n",
        "sequences = tokenizer.texts_to_sequences(data.text)\n",
        "reviews = pad_sequences(sequences, maxlen=max_len)\n",
        "print(reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cvZ7tMds4fIX"
      },
      "outputs": [],
      "source": [
        "data['label'] = data['label'].map({'Negative': 0, 'Positive': 1})\n",
        "labels=to_categorical(data.label,num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "U_bcQA5J4f-A"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.1, stratify=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQsrczQR4hL-",
        "outputId": "c14b88ca-aa5c-40d0-81eb-d816c75feea4"
      },
      "outputs": [],
      "source": [
        "model3 = Sequential()\n",
        "model3.add(layers.Embedding(max_words, 40, input_length=max_len)) # optimize hyper parameters\n",
        "model3.add(layers.Bidirectional(layers.LSTM(20,dropout=0.6)))\n",
        "model3.add(layers.Dense(2,activation='softmax'))\n",
        "\n",
        "model3.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model3.fit(X_train,\n",
        "                     y_train,\n",
        "                     epochs=8,\n",
        "                     validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "X2YUj0FW4ivN",
        "outputId": "151dc539-9de8-469c-cbd3-5bf3cfd55bbd"
      },
      "outputs": [],
      "source": [
        "names=['text', 'label']\n",
        "df_test = pd.read_csv('drive/MyDrive/SUML_data/test_dataset.csv', names=names)\n",
        "data_test=df_test[['text','label']]\n",
        "clean_data(data_test, re_letters)\n",
        "data_test['text']=data_test['text'].apply(remove_stopwords)\n",
        "data_test['text']=data_test['text'].progress_apply(lemmatize)\n",
        "data_test.sample(5)\n",
        "sns.countplot(x='label',data=data_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXtF6iKLi9lW"
      },
      "outputs": [],
      "source": [
        "#load the pre-trained model\n",
        "current_model = load_model('drive/MyDrive/SUML_data/18_01_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2ZrYA0M4jv1"
      },
      "outputs": [],
      "source": [
        "# Make predictions on new text data using the pre-trained model\n",
        "new_sequences = tokenizer.texts_to_sequences(data_test.text)\n",
        "new_data = pad_sequences(new_sequences, maxlen=max_len)\n",
        "new_predictions = current_model.predict(new_data)\n",
        "\n",
        "score = 0\n",
        "print(len(data_test))\n",
        "\n",
        "# Print the predicted labels for new text data\n",
        "for i, prediction in enumerate(new_predictions):\n",
        "    if prediction[1] > prediction[0]:\n",
        "      label = 1\n",
        "    else:\n",
        "      label = 0\n",
        "\n",
        "    if (data_test.label[i] == label):\n",
        "      score = score + 1\n",
        "\n",
        "print(score)\n",
        "print(score / len(data_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQhQPtOD2Uat"
      },
      "source": [
        "SAVING THE MODEL USING PICKLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bAz9mLz72HfV"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A9dvBxw2T4N"
      },
      "outputs": [],
      "source": [
        "filename = 'model30122024pickle.sav'\n",
        "pickle.dump(model3, open(filename, 'wb'))\n",
        "\n",
        "# save the model\n",
        "model3.save('model30122024keras.keras')\n",
        "model3.save('model30122024.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT4TTbQX4vle"
      },
      "outputs": [],
      "source": [
        "current_model = pickle.load(open('model30122024pickle.sav', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "X-wB5DcQ4tUJ",
        "outputId": "da35c7de-5a3f-4ada-fc71-93cab81296e5"
      },
      "outputs": [],
      "source": [
        "# Make predictions on new text data using the pre-trained model\n",
        "while True:\n",
        "    user_input = input(\"Enter a review (or 'exit' to end): \")\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # Tokenize and pad the user input\n",
        "    user_sequence = tokenizer.texts_to_sequences([user_input])\n",
        "    user_data = pad_sequences(user_sequence, maxlen=max_len)\n",
        "\n",
        "    # Make predictions for the user input\n",
        "    user_prediction = current_model.predict(user_data)[0]\n",
        "    print(user_input)\n",
        "    print(user_data)\n",
        "    # Print the predicted label for the user input\n",
        "    if user_prediction[1] > user_prediction[0]:\n",
        "        predicted_label = 1\n",
        "    else:\n",
        "        predicted_label = 0\n",
        "\n",
        "    print(\"Predicted Label:\", predicted_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EqFB2jFg_h5x"
      },
      "outputs": [],
      "source": [
        "#save the tokenizer\n",
        "import pickle\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iszzrn8b_N3Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
